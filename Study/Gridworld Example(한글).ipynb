{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gridworld Example\n",
    "<br />\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/73898997-934d4900-48ce-11ea-8ca5-870364ecd377.png)\n",
    "\n",
    "<br /><br />\n",
    "동적 프로그래밍 환경에서, 에이전트는 이 보상 구조를 알고 있고, 상태 간에 전환이 어떻게 일어나는지 알고 있다. 그래서 에이전트는 이미 환경이 어떻게 작동하는지 모든 것을 알고 있다. 에이전트는 어떻게 이 정보를 사용하여 최적의 정책을 찾을 수 있을까?\n",
    "\n",
    "### An Iterative Method\n",
    "\n",
    "<br />\n",
    "Gridworld의 예를 들어 특정 정책에 해당하는 가치 함수를 어떻게 결정할 수 있는지 알아보자. 우선, 상태를 나열할 것이다. 에이전트가 수행가능한 행동 집합(Action Space)에서 동일한 확률로 행동을 선택하는 확률적인 정책(Stochastic Policy)을 평가하려고 한다고 하자. 예를 들어 상태 $S_1$에서는 에이전트가 오른쪽 또는 아래로 움직이는데 이는 본질적으로는 동전 던지기로 결정하는 것과 같다.\n",
    "<br /><br />\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/73899460-0f945c00-48d0-11ea-9dcc-83053a95391e.png)\n",
    "#### 그렇다면 이 정책에 해당하는 상태-가치 함수는 어떻게 결정할 수 있을까?\n",
    "\n",
    "<br />\n",
    "우선 상태 $S_1$에 대해 알고 있는 내용을 기록하는 것으로 시작한다. 그러면 50%의 확률로 에이전트는 오른쪽으로 움직인다. 에이전트가 오른쪽으로 움직이는 이벤트의 경우, 예상 반환값은(Expected Return)$-1$에다 이동한 상태인 $S_2$의 상태 값을 더하여 계산할 수 있다.\n",
    "* $(1+v_\\pi(s_2))$\n",
    "\n",
    "<br />\n",
    "또 다른 50%의 확률로, 에이전트는 아래로 이동하고 그에 따른 예상 반환값은\n",
    "* $(-3+v_\\pi(s_3))$\n",
    "\n",
    "<br />\n",
    "따라서 상태 $S_1$의 값은 이 두 값의 평균으로 계산할 수 있다. 에이전트가 *동일한 확률*로 수행가능한 행동을 선택하므로, 아마 너는 이 방정식을 상태 $S_1$에서 평가된 **벨만 방정식**으로 인식할 수도 있겠다.\n",
    "* $v_\\pi(s_1) = {1 \\over 2}(-1 +v_\\pi(s_2))+{1 \\over 2}(-3+v_\\pi(s_3))$\n",
    "\n",
    "<br />\n",
    "여기서 우리는 상태 $S_1$의 값을 이어지는 가능한 상태들의 값으로 표현할 수 있다는 맥락을 이해했을 것이다. 상태 $S_2$에 대해 이어서 살펴보면, 에이전트가 왼쪽으로 이동하면 예상 반환값이\n",
    "* $(-1+v_\\pi(s_1))$\n",
    "\n",
    "<br />\n",
    "그리고 에이전트가 아래로 이동할 경우 기대되는 반환값은 5에다가 터미널 상태 값($S_4$)을 더한 \n",
    "* $(5+v_\\pi(s_4))$\n",
    "\n",
    "<br />\n",
    "그리고 상태 $S_2$의 값을 얻으려면 이 두 값의 평균만 취하면 된다. 그리고 우리는 또 벨만 방정식에 도달했지만 지금은 상태 $S_2$라는 점을 기억하자. 우리는 위에서 했던 방식과 같은 방식으로 상태 $S_3$에서 에이전트는 위로 또는 오른쪽으로 이동할 수 있다는 점을 고려하여 평균내는 방식으로 상태 $S_3$의 값을 구할 수 있다.\n",
    "* $v_\\pi(s_2) = {1 \\over 2}(-1+v_\\pi(s_1))+{1\\over2}(5+v_\\pi(s_4))$\n",
    "* $v_\\pi(s_3) = {1 \\over 2}(-1+v_\\pi(s_1))+{1\\over2}(5+v_\\pi(s_4))$\n",
    "\n",
    "<br />\n",
    "마지막으로, 상태 $S_4$는 끝나는 상태(Terminal state)이므로 값은 항상 0이다.\n",
    "* $v_\\pi(s_4) = 0$\n",
    "\n",
    "<br />\n",
    "이 식들을 풀면, 각 상태의 값을 구할 수 있다.\n",
    "* $v_\\pi(s_1) = 0$\n",
    "* $v_\\pi(s_2) = 2$\n",
    "* $v_\\pi(s_3) = 2$\n",
    "* $v_\\pi(s_4) = 0$\n",
    "\n",
    "<br />\n",
    "각 상태에 대해, 우리는 이제 에이전트가 해당 상태에서 시작하고 동일한 확률로 무작위 정책을 따르는 맥락으로 예상 반환값을 구할 수 있다.\n",
    "<br /><br />\n",
    "여기서 유일한 문제는 일반적으로 상태 공간이 훨씬 넓기 때문에 방정식 체계를 직접 푸는 것은 어렵다는 것이다. 이 경우 **iterative method**을 사용하여 방정식을 해결하는 것이 더 효과적이다. 그래서 우리가 할 수 있는 것은 시스템을 직접 해결하는 대신 각 상태의 값을 추측하는 것으로 출발하는 것이다. 일반적으로 각 상태의 값을 0으로 설정한다. 그리고 우리는 추측을 업데이트 시키고자 한다. 이를 위해 이전에 본 **벨만 방정식**을 사용하지만, 아래와 같은 업데이트 규칙으로 작동하도록 채택한다. 우리는 이 업데이트 규칙을 사용하여 상태 값에 대한 새로운 추측을 얻게 되는 것이다.\n",
    "<br /><br />\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/73905848-11681a80-48e4-11ea-8b45-2f981e9a2bb8.png)\n",
    "\n",
    "<br /><br />\n",
    "상태 $S_4$의 값은 항상 0일 것이기 때문에 갱신할 필요가 없다. 그러나 우리는 첫 번째 상태로 되돌아가서 바뀐 다른 상태의 추측값을 이용하여 처음 설정한(0) 추측을 업데이트하는 방식으로 계속 진행할 수 있다. 마지막으로, 이 반복 알고리즘(iterative algorithm)은 참값 함수에 수렴하는 추정치를 산출한다. 이것은 ‘반복 정책 평가(iterative policy evaluation)’라는 알고리즘의 이면에 있는 아이디어로, 뒤이어 살펴볼 것이다.\n",
    "<br />\n",
    "위의 내용을 조금 더 이해하기 위해 벨만 기대 방정식이 상태 $s_1$에 대해 무엇을 알려주는 지 살펴보자.<br /><br />\n",
    "$v_\\pi(s_1) = \\sum_{a \\in {\\mathcal{A}(s_1)}}\\pi(a|s_1)\\sum_{s'\\in \\mathcal{S},r \\in \\mathcal{R}}p(s',r|s_1,a)(r+\\gamma v_\\pi(s'))$\n",
    "<br /><br />\n",
    "그러면, 우리는 이제 상태 $s_1$에 관한 방정식들을 끌어낼 수 있는데:\n",
    "* $\\mathcal{A}(s_1)$ = ${down, right}$ (상태 $s_1$에서 에이전트는 두 가지 행동만 취할 수 있다: 아래로 혹은 오른쪽 이동) <br /><br />\n",
    "\n",
    "* $\\pi(down|s_1) = \\pi(right|s_1) = {1\\over 2}$ (우리는 현재 상태 $s_1$에서 에이전트가 아래로 갈 확률이 50%이고 오른쪽으로 갈 확률이 50%인 정책을 살펴보고 있다.)<br /><br />\n",
    "\n",
    "* $p(s_3,-3|s_1,down) = 1$ (and $p(s',r|s_1,down) = 0$ if ${s' \\ne s_3} $or$ {r \\ne -3})$ (만약 에이전트가 상태 $s_1$에서 아래로 가는 것을 선택한다면 100%의 확률로 상태 $s_3$으로 이동하게 되고 -3 만큼의 보상을 받게 된다.)<br /><br />\n",
    "\n",
    "* $p(s_2,-1|s_1,right) = 1$ (and $p(s',r|s_1,right) = 0$ if ${s' \\ne s_2} $or$ {r \\ne -1})$ (만약 에이전트가 상태 $s_1$에서 오른쪽으로 가는 것을 선택한다면 100%의 확률로 다음 상태는 $s_2$가 되고, -1만큼의 보상을 받게 된다.)<br /><br />\n",
    "\n",
    "* $\\gamma = 1$ (이 gridworld 예시에서 할인율은 1로 설정한다.)\n",
    "\n",
    "<br />\n",
    "\n",
    "### Iterative Policy Evaluation\n",
    "<br />\n",
    "반복 정책 평가 알고리즘은 에이전트가 이미 환경을 특징짓는 MDP(마르코프 프로세스)에 대한 완전하고 완벽한 지식을 갖추고 있다고 가정한다. 그리고 이 알고리즘은 상태-가치 함수에 대한 벨만 기대 방정식을 모티브로 만들어졌다. 아래 식에서 $V$는 정책 가치 함수에 대한 현재의 추정을 나타낸다. 그리고 이 알고리즘의 좋은 점은 몇 가지 기술적 조건이 충족되는 한 올바른 가치 함수로 수렴될 것을 보장한다는 것이다.\n",
    "<br /><br />\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/73907663-8cccca80-48ea-11ea-8ecd-9e3fa5d24e12.png)\n",
    "![image](https://user-images.githubusercontent.com/46597294/73907785-f056f800-48ea-11ea-8df3-04e817fe6a0f.png)\n",
    "\n",
    "<br /><br />\n",
    "하지만 실제 수렴값을 얻기 위해 알고리즘을 무한히 반복하는 것은 좋아보이지 않는다. 적당히 수렴값 근처로 갔을 때 멈추어야 하는 데, 어디가 근처인지 알 방도가 있냐는 것이다. 실제로 알고리즘을 적용하게 되면 처음에 몇 번 반복을 통한 업데이트과정을 보면 크게 크게 값이 갱신되는 것을 볼 수 있지만, 나중에 시간이 지날수록 그 변화의 폭은 알아채기 힘들 정도로 작아진다.  그때가 바로 수렴값 근처에 갔다고 합리적으로 판단할 수 있는 시점인 것이다.\n",
    "<br /><br />\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/73908249-70319200-48ec-11ea-9497-42c5a0cc096a.png)\n",
    "\n",
    "<br /><br />\n",
    "정책 평가(policy evaluation)는 각 상태 $s \\in \\mathcal{S}$ 에 대해 $v_\\pi(s)$가 유한한 한 정책 $\\pi$하에서의 상태-가치 함수 $v_\\pi(s)$로 수렴하게 되어있다. 유한한 마르코프 결정 프로세스(finite MDP)는 아래 조건 하에서 수렴을 보장한다.\n",
    "* $\\gamma < 1$ 혹은\n",
    "* 만약 에이전트가 어떤 상태에서 출발한다 했을 때 ${s\\in\\mathcal{S}}$, 정책을 따를 경우 반드시 최종적으론 터미널 상태에 도달해야 한다.\n",
    "\n",
    "### Action values\n",
    "앞 부분에서 *상태-가치 함수 $v_\\pi$* 를 추정하기 위해 반복 정책 평가의 구현(미니 프로젝트)을 해보았다. 이제 간단한 GridWorld 예시를 사용하여 *상태 가치 함수 $v_\\pi$ 을(를) 행동 가치 함수 $q_\\pi$* 로 변환하는 것에 대해 알아볼 것이다.\n",
    "반복적인 정책 평가를 설명하기 위해 사용했던 작은 Gridworld를 떠올려보자. 아래 그림은 무작위 정책(equiprobable random policy)에 대한 상태 가치 함수를 나타낸다.<br /><br />\n",
    "![image](https://user-images.githubusercontent.com/46597294/74000781-aafaff00-49ac-11ea-9ff3-87d69302ec37.png)\n",
    "\n",
    "<br /><br />\n",
    "같은 정책 하에서 위 상태-가치 함수에 대한 행동-가치 함수가 아래에 나타나있다. 찬찬히 생각해보자.<br /><br />\n",
    "![image](https://user-images.githubusercontent.com/46597294/74000848-e1387e80-49ac-11ea-9afd-9f7bcbc6b720.png)\n",
    "\n",
    "<br /><br />\n",
    "예를 들어, $q_\\pi(s_1, right)$ 를 보면, 이 행동-가치 값은 아래와 같이 계산된다.\n",
    "* $q_\\pi(s_1, right) = -1 + v_\\pi(s_2) = -1 + 2 = 1,$\n",
    "\n",
    "여기서 우리는 상태-행동 쌍 $s_1, \\text{right}$  을 두 가지 값 (1) 우측으로 이동함($s_2$에 도달)에 따른 즉시 보상과 (2) 에이전트가 $s_2$에서 출발하여 현 정책을 따를 경우 획득한 누적 보상. 의 합으로 나타낼 수 있다는 것을 알 수 있다.\n",
    "\n",
    "### Far More Complex Environments\n",
    "\n",
    "이 간단한 Gridworld의 예에서, 환경은 결정론적(deterministic)이다. 즉, 에이전트가 선택한 행동 이후 다음 상태와 보상은 결정되어 있다(비랜덤적). 결정론적 환경의 경우,\n",
    "* $p(s',r|s,a) \\in {0,1} $ for all $s',r,s,a$\n",
    "\n",
    "> 이 경우, 에이전트가 상태 $s$에서 행동 $a$를 취했을 때, 다음 상태 $s'$와 보상 $r$은 확실히 정해져있고, $q_\\pi(s,a) = r + \\gamma v_\\pi(s')$ 이다.\n",
    "\n",
    "일반적으로, 환경이 항상 결정론적이지 않고 확률적(stochastic)인 경우가 많다. 미니 프로젝트에서 다루었던 FrozenLake 환경이 그러하다. 이 경우, 에이전트가 행동을 선택하면 그에 따른 상태와 보상이 확실하게 정해져있는 것이 아닌 (조건부)확률분포 $p(s',r|s,a)$ 를 따른다. \n",
    "> 이 경우, 에이전트가 상태 $s$에서 행동 $a$를 취했을 때, 다음 상태 $s'$로 이동하고 보상 $r$을 받을 확률은 $p(s',r|s,a)$ 이다. 이때, $q_\\pi(s,a) = \\sum_{s' \\in \\mathcal{S}^+, r \\in \\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$ 이 $r + \\gamma v_\\pi(s')$ 합에 대한 기댓값임을 알 수 있다.\n",
    "\n",
    "\n",
    "### Implementation: Estimation of Action Values\n",
    "\n",
    "다음에 우리는 상태 가치 함수 $v_\\pi$의 측정값 $V$과 one-step dynamics of MDP $p(s',r|s,a)$ 을 인풋으로 하고, 행동-가치 함수 $q_\\pi$ 의 측정값 $Q$를 반환하는 알고리즘을 살펴볼 것이다. 이를 위해, 이전에 배웠던 $v_\\pi$로 부터 $q_\\pi$를 얻을 수 있는 one-step dynamics $p(s',r|s,a)$ of the Markov decision process (MDP)을 사용해야 한다.\n",
    "\n",
    "* $q_\\pi(s,a) = \\sum_{s' \\in \\mathcal{S}^+, r \\in \\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/74002273-15fb0480-49b2-11ea-9e64-83049fb92f9b.png)\n",
    "\n",
    "<br />\n",
    "\n",
    "### Policy Improvement\n",
    "\n",
    "정책평가(Policy evaluation)서 에이전트는 MDP에 대한 충분한 지식이 있어야 한다는 것만 명심하고, 일단은 최적의 정책을 찾아보자. 가장 좋은 정책을 찾아내기까지 정책 평가는 부분적으로 후보 정책들을 평가할 수 있는 데 도움이 된다. 가치 함수를 사용하여 새롭고 이전의 정책에 비해 나은 정책을 제안하는 정책 갱신(Policy improvement) 알고리즘을 이제 알아보도록 하자. <br /><br />\n",
    "정책 평가는 정책을 취하여 가치 함수를 반환한다. 그러면 우리는 동일한 가치 함수를 가지고 정책 갱신을 통해 (잠재적으로) 새롭고 개선된 정책을 얻을 수 있다. 그 다음, 그 새로운 정책을 가지고 다시 정책 평가를 하고, 또 정책 갱신을 하여 최적의 정책으로 수렴하기 까지가 가능해진다. 좀 더 이해하기 위해 앞에서 봤던 Gridworld의 맥락에서 살펴보자. 이 예제에서는 터미널 상태가 유일하게 오른쪽 아래에 있는 episodic task라는 것을 기억하자.  에이전트가 이미 환경에 대해 모든 것을 알고 있다고 가정하기 때문에 에이전트가 이동함에 따른 변화와 보상이 어떻게 결정되는 지 알고 있다. 그리고 특히, 이런 정보를 알기 위해 환경과의 상호작용이 불필요하다. 하지만 여기서도 최적의 정책을 결정할 필요는 있다. 따라서 에이전트가 최적의 정책을 찾기 위해 일단 추측부터 시작한다고 하자. 앞에서 우리는 동일한 확률의 무작위 정책(수행가능한 행동 집합에서 동일한 확률로 행동을 선택하는)으로 시작하는 것이 합리적이라고 배웠다. 이 아이디어를 기반으로, 에이전트는 반복 정책 평가를 통해 해당 가치 함수를 계산할 수 있다. <br /><br />\n",
    "그래서 이제 질문은 **이 정책 개선 step을 어떻게 설계해야 적어도 현재와 같은(이상) 수준의 정책을 찾을 수 있을까?** 정책 개선은 두 단계로 나뉘는데, 첫 번째 단계는 상태-가치 함수를 행동-가치 함수로 변환하는 것이다. 행동-가치 함수를 얻은 후에는 이 행동-가치 함수를 사용하여 동일한 확률의 무작위 정책보다 나은 정책을 얻는 데에만 집중하면 된다. 자, 계속 집중해보자. 각 상태에 대해, 우리는 행동 가치 함수를 극대화하는 행동을 선택할 것이다. 따라서 왼쪽 상단 모서리에 있는 상태(Gridworld 예제)부터 시작하여 1은 (-1)보다 크므로 정책은 아래쪽 대신 오른쪽을 선택할 것이다. 이 시점에서 행동-가치 함수를 최대화하는 행동이 여러가지일 경우엔 어떻게 할 수 있을까? 이 경우에는 선택권이 있다. 무작위로 선택하거나 확률적인 정책을 만들어 결정할 수 있다. <br /><br />\n",
    "왜 이 알고리즘이 작동할 수 있는 지 깊게 파헤쳐 보자. 에이전트가 모든 시간(time step)에 대해 같은 정책을 따르는 경우 정책에 해당하는 가치 함수는 기댓값을 저장하기만 하면 된다. 에이전트가 시간 t에서 행동 a를 선택하고 현 정책을 따랐을 때 어떤 일이 벌어지는 지를 보면 행동-가치 함수값을 계산할 수 있다. 그리고 다음 정책($\\pi'$)을 구축하기 위해 이 행동-가치 함수값을 보고 이를 극대화하는 첫 행동을 선택할 수 있다. 이에 따라 에이전트는 어떤 (행동-가치 함수의) 기댓값이 (모든 시간 t에 대해) 이전의 정책과 상관없이 에이전트는 $\\pi'$를 따름으로써 높은 기댓값을 가지게 된다. 어떤 정책이 이전의 정책보다 나은지를 확인하기 위해선 모든 상태에 대한 행동-가치 함숫값이 크면 된다.\n",
    "<br /><br />\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/46597294/74004906-8574f200-49ba-11ea-8e13-4bf040625105.png)\n",
    "\n",
    "\n",
    "![image](https://user-images.githubusercontent.com/60770121/74006328-af301800-49be-11ea-9510-470818dedef6.png)\n",
    "\n",
    "\n",
    "<br /><br />\n",
    "정리하면, 먼저 상태-가치 함수로부터 행동-가치 함수를 계산한다. 그리고, 각 상태에 대해 나은 정책을 구축하기 위해, 우리는 행동-가치 함수값을 극대화하는 행동을 고른다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
